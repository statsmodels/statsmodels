{
 "metadata": {
  "name": "example_gmle"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Generic Maximum Likelihood Model\n",
      "\n",
      "This tutorial explains how to quickly implement new maximum likelihood models in `statsmodels`. The `GenericLikelihoodModel` class eases the process by providing tools such as automatic numeric differentiation and a unified interface to ``scipy`` optimization functions. Using ``statsmodels``, users can fit new MLE models simply by \"plugging-in\" a log-likelihood function. \n",
      "\n",
      "## Example: Negative Binomial Regression for Count Data\n",
      "\n",
      "Consider a negative binomial regression model for count data with\n",
      "log-likelihood (type NB-2) function expressed as:\n",
      "\n",
      "$$\n",
      "    \\mathcal{L}(\\beta_j; y, \\alpha) = \\sum_{i=1}^n y_i ln \n",
      "    \\left ( \\frac{\\alpha exp(X_i'\\beta)}{1+\\alpha exp(X_i'\\beta)} \\right ) -\n",
      "    \\frac{1}{\\alpha} ln(1+\\alpha exp(X_i'\\beta)) \\\\\n",
      "$$\n",
      "$$\n",
      "    + ln \\Gamma (y_i + 1/\\alpha) - ln \\Gamma (y_i+1) - ln \\Gamma (1/\\alpha)\n",
      "$$\n",
      "\n",
      "with a matrix of regressors $X$, a vector of coefficients $\\beta$,\n",
      "and the negative binomial heterogeneity parameter $\\alpha$. \n",
      "\n",
      "Using the ``nbinom`` distribution from ``scipy``, we can write this likelihood\n",
      "simply as:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy.stats import nbinom\n",
      "def _ll_nb2(y, X, beta, alph):\n",
      "    mu = np.exp(np.dot(X, beta))\n",
      "    size = 1/alph\n",
      "    prob = size/(size+mu)\n",
      "    ll = nbinom.logpmf(y, size, prob)\n",
      "    return ll"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## New Model Class\n",
      "\n",
      "We create a new model class which inherits from ``GenericLikelihoodModel``:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from statsmodels.base.model import GenericLikelihoodModel\n",
      "class NBin(GenericLikelihoodModel):\n",
      "    def __init__(self, endog, exog, **kwds):\n",
      "        super(NBin, self).__init__(endog, exog, **kwds)\n",
      "    def nloglikeobs(self, params):\n",
      "        alph = params[-1]\n",
      "        beta = params[:-1]\n",
      "        ll = _ll_nb2(self.endog, self.exog, beta, alph)\n",
      "        return -ll \n",
      "    def fit(self, start_params=None, maxiter=10000, maxfun=5000, **kwds):\n",
      "        if start_params == None:\n",
      "            # Reasonable starting values\n",
      "            start_params = np.append(np.zeros(self.exog.shape[1]), .5)\n",
      "            start_params[0] = np.log(self.endog.mean())\n",
      "        return super(NBin, self).fit(start_params=start_params, \n",
      "                                     maxiter=maxiter, maxfun=maxfun, \n",
      "                                     **kwds) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "url = 'http://vincentarelbundock.github.com/Rdatasets/csv/COUNT/medpar.csv'\n",
      "medpar = pd.read_csv(url)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mod = NBin(y, X)\n",
      "res = mod.fit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 3.209014\n",
        "         Iterations: 805\n",
        "         Function evaluations: 1238\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from statsmodels.iolib.summary import Summary\n",
      "res.model.exog_names.append('alpha') # negbin het parameter must be named\n",
      "smry = Summary()\n",
      "smry.add_base(res)\n",
      "print smry"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "               Results: Negative binomial model\n",
        "===============================================================\n",
        "Model:              NBin             No. Iterations: 805.0000  \n",
        "Dependent Variable: los              Scale:          1.0000    \n",
        "Date:               2013-02-03 09:18 AIC:            9606.9532 \n",
        "No. Observations:   1495             BIC:            9638.8125 \n",
        "Converged:          1.0000           Log-Likelihood: -4797.4766\n",
        "---------------------------------------------------------------\n",
        "               Coef.  Std.Err.    t      P>|t|    [0.025 0.975]\n",
        "---------------------------------------------------------------\n",
        "Intercept      2.3103   0.0679 33.9998    <2e-16  2.1771 2.4434\n",
        "type2          0.2213   0.0506  4.3734 1.223e-05  0.1221 0.3204\n",
        "type3          0.7061   0.0761  9.2754    <2e-16  0.5569 0.8554\n",
        "hmo           -0.0680   0.0533 -1.2764    0.2018 -0.1724 0.0364\n",
        "white         -0.1290   0.0685 -1.8826    0.0598 -0.2634 0.0053\n",
        "alpha          0.4458   0.0198 22.4952    <2e-16  0.4069 0.4846\n",
        "===============================================================\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 12
    }
   ],
   "metadata": {}
  }
 ]
}